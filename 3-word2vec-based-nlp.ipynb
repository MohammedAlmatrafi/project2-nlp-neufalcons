{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bow\n",
    "import w2v\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ma0me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ma0me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ma0me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ma0me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              title  \\\n",
       "0      1  As U.S. budget fight looms, Republicans flip t...   \n",
       "1      1  U.S. military to accept transgender recruits o...   \n",
       "2      1  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3      1  FBI Russia probe helped by Australian diplomat...   \n",
       "4      1  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
       "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
       "\n",
       "                 date  \n",
       "0  December 31, 2017   \n",
       "1  December 29, 2017   \n",
       "2  December 31, 2017   \n",
       "3  December 30, 2017   \n",
       "4  December 29, 2017   "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/ironhack-labs/project-nlp-challenge/refs/heads/main/dataset/data.csv\"\n",
    "\n",
    "# Read CSV file\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label      0\n",
       "title      0\n",
       "text       0\n",
       "subject    0\n",
       "date       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of duplicate rows in training data: 201\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNumber of duplicate rows in training data:\", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ma0me\\.conda\\envs\\tnsr-gpu\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_x, test_x, train_y, test_y = bow.split_train_test(df)\n",
    "\n",
    "_, vectorizer = bow.create_features(train_x)\n",
    "tokenized_train = bow.tokenize_texts(train_x, vectorizer)\n",
    "tokenized_test = bow.tokenize_texts(test_x, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "word2vec_model_google = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    './archive/GoogleNews-vectors-negative300.bin', \n",
    "    binary=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_embed = w2v.embed_texts(tokenized_train, word2vec_model_google)\n",
    "test_x_embed = w2v.embed_texts(tokenized_test, word2vec_model_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text\n",
       "Coming off from her latest record-breaking album and a performance at the GRAMMYs, singer/songwriter extraordinaire Adele decided to take some downtime and enjoy Disneyland with her partner Simon Konecki and son Angelo.Her security detail must have been fierce, and day guided by Disney s wonderful cast members, but as you can see in the picture below, she and her little family seemed to have the time of their lives strolling through the park and clearly buying up a lot of the merchandise.Something else was noticed in the photos, too. Adele s son appears to be enjoying his fun-filled day at the park dressed as  Anna  from Disney s mega-hit Frozen, complete with dress and ballet flats. It seems as if the singer is allowing her son to completely be himself and choose what he wants to like and enjoy.1. Adele let her son dress up as Anna from Frozen. Amazing. 2. Peter is smiling cuz Disneyland pic.twitter.com/NwWb4BSN8C  Marc (@MarcMonster) February 16, 2016This, of course, isn t the first time the superstar made it known that she s supportive of her son no matter what. Back in 2012 she told TIME magazine that she ll always accept him. He makes me so proud of myself, and he makes me like myself so much. I ve never not liked myself. I don t have hangups like that. But I m so proud of myself that I made him in my belly   I can t wait to know who his best friends are going to be, who his girlfriend or his boyfriend is going to be or what movies he likes   Whatever my kid wants to do or be I will always support him no matter what. And she s clearly holding true to her word.Adele s son Angelo isn t the only little guy who enjoys the amazing fashion of Frozen. Just last fall, another parent made news on Facebook when he posted a picture of his son in his Halloween costume of choice as  Elsa. Here s the thing about childhood. It s important to let your kids be kids and have fun, and Adele seems to perfectly be doing just that.Featured image: DailyMail                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              143\n",
       "MOSCOW (Reuters) - Russian Deputy Foreign Minister Gennady Gatilov said on Monday that U.N. mediator on Syria Staffan de Mistura might visit Moscow before the end of the year, RIA news agency cited Gatilov as saying.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            19\n",
       "Besty Devos is Trump s conservative choice for Education Secretary. During her confirmation process leftist Bernie Sanders  most important question had absolutely nothing to do with providing our kids the best education possible. Sanders had one burning question for Betsy Devos, and it had to do with what else? How can we give Americans something for free?Betsy Devos  answer to Bernie was perfect                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     27\n",
       "Sarah Palin is an idiot. While this has been commonly accepted ever since she slinked out of her slime pit and emerged on the political scene, Palin recently managed to be so utterly stupid that it prompted an official White House response. After Palin s son Track drunkenly beat his girlfriend and made threats with a semiautomatic weapon, Caribou Barbie did the only thing she knows how to do: she blamed that socialist Communist Muslim atheist fascist dictator Barack Obama.Sure, the half-term, half-wit former Governor of Alaska could have had a moment of self-reflection, questioning her role as a parent and what she did wrong in failing to instill proper values in her precious little snowflake. She could have spoken out against domestic violence in general, and pointed out that it is an issue that faces many families, including hers. She could have said nothing at all. Instead, she told a crowd at a Donald Trump rally that Obama gave her son PTSD when he served in Iraq for a single year under Bush, and blamed Track s violent tendencies the President s blatant attempt to deprive veterans of medical and psychological treatment (failing to mention that  it was actually Republicans who slashed the budget for veterans  care): I can talk personally about this, I guess it s kind of the elephant in the room. My son like so many others, they come back a bit different, they come back hardened, they come back wondering if there is that respect for what it is that their fellow soldiers and airmen and every other member of the military so sacrificially have given to this country. And that starts from the top. It s a shame that our military personnel even have to wonder, if they have to question, if they re respected anymore. It starts from the top. The question though that comes from our own president where they have to look at him and wonder, do you know what we go through? Do you know what we re trying to do to secure America and to secure the freedoms that have been bequeathed us? I can certainly relate with other families who kind of feel these ramifications of some PTSD and some of the woundedness that our soldiers do return with,  Palin said.  And it makes me realize more than ever it is now or never for the sake of Americas finest that we have that commander in chief that will respect them and honor them. Earnest said that while it s easy to mock Palin   really easy   the topics she discussed are not a joke: The reaction of some people I think is to make light of some of the rhetoric that we see on the campaign trail, particularly from Gov. Palin. But the fact is domestic violence is not a joke. Gun violence is not a joke. Problems with addiction are not a joke. The consequences, or I should say, the sacrifices that many of our men and women in uniform make for our safety and security are not a joke. We take [those issues] very seriously and there are many communities and families that are dealing with these very difficult challenges in a way that is sometimes difficult to talk about publicly,  he said.Classy   something one would never expect from someone who would fling herself on top of a dogpile in a drunken brawl her family started at someone s birthday party.Watch Earnest eloquently destroy her below:[youtube https://www.youtube.com/watch?v=aHuDTaEGJ2s&w=560&h=315]Featured image via screengrab (1,2)    236\n",
       "Donald Trump becomes more unhinged by the day. The week we just witnessed is his craziest yet. First, there was the hyper-partisan, free-wheeling speech he gave to a bunch of children at the Boy Scouts  Jamboree. The speech outraged so many people that the Boy Scouts of America had to apologize. Then, while speaking in front of police officers in Long Island, New York, Trump openly endorsed and encouraged police brutality in a racist speech sure to fire up his mouthbreathing base. And of course, there was the ouster of White House Chief of Staff Reince Priebus via Twitter, who he humiliated as he was pushed out the door. To top it all off, he called for banning transgender soldiers from the military without even telling the Pentagon and the Joint Chiefs of Staff first. This major, sweeping policy change was also made via Twitter.Through all of this madness, there were, of course, Trump s repeated and sustained attacks on his own Attorney General, Jeff Sessions. Even though this should have Republicans reeling with outrage, and some Capitol Hill Republicans have fired warning shots at Trump about Sessions, there is one person who has been suspiciously silent throughout the chaos: Mike Pence, Trump s second-in-command.Well, Rep. Maxine Waters (D-CA) thinks she knows the reason for that. Taking to Twitter on Saturday evening, Rep. Waters said what anyone watching is likely thinking: Pence is just riding out the Trump insanity until the Republicans on Capitol Hill finally decide they ve had enough, and they pull the trigger on Trump. This, of course, would make Pence President. Here is that tweet:Mike Pence is somewhere planning an inauguration. Priebus and Spicer will lead the transition.  Maxine Waters (@MaxineWaters) July 30, 2017Rep. Waters is likely right. There s no way establishment Republicans like Mike Pence actually like living with Donald Trump as president. He is, and always has been, nothing more than a vehicle to put them in power. And honestly, because of the chaos, Trump has exactly zero legislative achievements. Sure, Jeff Sessions is doing awful things from the DOJ, but those horrific policies can be promptly reversed once a new administration arrives. Trump is ignorant, incompetent, and crazy. Pence, however, is much more dangerous. I can t believe I m saying this, but I truly hope Rep. Waters  predictions never come to pass. America will be an Evangelical theocracy in no time under Pence.In short, either way, America, we re screwed.Featured image via Chip Somodevilla/Getty Images                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       203\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ... \n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   300\n",
       "CHARLOTTESVILLE, Va. (Reuters) - When white supremacists began rallying in downtown Charlottesville this weekend, Liz Licht kept the TV off, trying  to shield her three kids from the hate spewed on the streets of this normally quiet college town. But after learning that a 32-year-old woman who joined a counter-protest was killed by a man described as having neo-Nazi sympathies, Licht could no longer keep news of the violence from her nine-year-old son and seven-year-old twins. “Our son went to bed scared that night,” Licht said. “He said he never really knew evil existed until that day.” Licht joined other parents to call on the local school district to help Charlottesville children exposed to the hate and violence, especially as they leave the safe haven of home to start the school year.  “We want to work with them to develop buddy systems to pair them up with someone who is an immigrant or refugee,” Licht, 41, said on Tuesday as she stood near a pile of flowers marking the street where Heather Heyer was killed. “Make it hands-on, not just talking about it.” Charlottesville Public Schools officials said they are preparing specific plans on how to address the issue when students return to classes next week. School leaders are tweaking their plans for the new year and preparing teachers to handle students’ questions about the violence and hate speech, schools Superintendent Rosa Atkins said in an e-mail. “If we miss these steps, we will miss an opportunity for healing and growth,” Atkins said. Saturday’s rally was the latest in a series of demonstrations by white supremacists in Charlottesville in recent months. It deteriorated into street fighting that culminated in Heyer’s killing, allegedly by 20-year-old James Alex Field, who injured 19 other people by crashing his car into a counter-protest.  Psychologists often warn that young children can be traumatized by images of violence and urge parents to limit their exposure to news accounts of events like Saturday’s rally. But given the white nationalist ideology that drove the “Unite the Right” event, experts said parents and schools should talk directly with their children about their beliefs. “This is a really important teaching moment,” said Gail Saltz, a clinical associate professor of psychiatry at the Cornell School of Medicine. Schools in particular could use the incident as a way to teach students to cope with bullying, by stepping up to object to bullies, rather than being passive bystanders. “Any way that one can be helpful always relieves anxiety,” said Saltz. “You might say to a child that in your microcosm of school, it’s really important to make everyone feel respected.” Corey Eicher, 42, stopped with his daughters, aged seven and four, to leave flowers at the memorial for Heyer. He said he had tried to soothe his children’s fears by talking about the police and race. “We showed them that a lot of the police working that day were black, of every color,” Eicher said. “My older daughter is seven, so she kind of understands what is happening.” Lila’s reaction to Saturday’s events was brief: “Scary.”                                                                                                                                                                                                                                                         263\n",
       "WASHINGTON (Reuters) - The top Democrat on the House of Representatives Intelligence Committee called on Wednesday for the release of part of a government report on the Sept. 11 attacks, saying this would diminish speculation that the 28 pages contained proof of Saudi involvement. “The release of these pages will not end debate over the issue, but it will quiet rumors over their contents,” Representative Adam Schiff, the intelligence panel’s ranking Democrat, said in a statement. “As is often the case, the reality is less damaging than the uncertainty.” The still-classified section of the official report on the 2001 attacks is central to a dispute over whether Americans should be able to sue the Saudi Arabian government for damages. The Office of the U.S. Director of National Intelligence is reviewing the material to see whether it can be declassified. Congressional aides said members of the House intelligence committee had seen the report. President Barack Obama, who was visiting Saudi Arabia on Wednesday, said he would not sign legislation making its way through Congress that would allow such lawsuits, if the Saudis were found to have any responsibility. Schiff has repeatedly called for the declassification of the 28-page report section. In his statement on Wednesday, he suggested the release of a redacted version to help address speculation that they contained proof of official Saudi government or senior Saudi officials’ involvement in the attacks. The 9/11 commission investigated those claims and never found sufficient evidence to support them, Schiff said.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                121\n",
       "PARIS (Reuters) - President Emmanuel Macron on Monday named award-winning author Leila Slimani as France s top emissary for promoting the use of the French language. The appointment of the 36-year old Franco-Moroccan writer as emissary for Francophone affairs follows Macron s decision to name a TV presenter and well-known ecologist as environment minister and an Olympic fencer as sports minister, opening up government to civil society.  Slimani was propelled into the limelight when she won the prestigious Prix Goncourt last year for her novel  Chanson Douce , which translates as lullaby. This year, she published a book about sexuality in Morocco.   The Francophone affairs brief has in the past been a ministerial post or a junior minister position, and had often been occupied by career politicians.  Macron, whose upstart centrist party trounced France s traditional political forces in an election earlier this year, has named a series of newcomers to his government.  Nicolas Hulot was appointed energy and environment minister, while Laura Flessel-Colovic, an Olympic gold medalist, is minister for sports.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     95\n",
       "WASHINGTON (Reuters) - Democrats on a powerful U.S. congressional panel are questioning whether Canada-based Valeant Pharmaceuticals may be wrongfully withholding documents in connection with its ongoing probe into sky-rocketing drug prices, according to an internal memo seen by Reuters on Tuesday. The memo, which was sent to Democratic members of the House of Representatives Committee on Oversight and Government Reform from the staff of its top Democrat Elijah Cummings, reveals that Valeant previously withheld readily available analyst reports prepared by banks such as Goldman Sachs, saying they were protected by attorney-client privilege. The memo says it also raises questions about other documents that are still being withheld, saying some of these “were not drafted by attorneys and do not include communications with attorneys.” A spokeswoman for Valeant denied that the company has done anything wrong, saying it has already provided more than 78,000 pages of documents and will continue cooperating. “We continue to discuss with the committee the issue of privileged documents, but any suggestion that we have withheld documents inappropriately is incorrect,” Laurie Little said in a statement. The April 26 memo comes just one day before three of Valeant’s top executives are slated to appear before a different U.S. Senate panel that is also investigating high drug prices. The company’s outgoing CEO Michael Pearson will be testifying before the Senate Special Committee on Aging late Wednesday afternoon, along with activist investor William Ackman, a majority shareholder and board member, and Howard Schiller, a director and former chief financial officer. Their appearance before Congress comes at an awkward and difficult time for the company, which is under fire from its shareholders and facing a number of ongoing government investigations into drug pricing and distribution, accounting and disclosures, and antitrust matters. Last month, the company announced that Pearson would be stepping down and that Ackman would be joining its board, after a board committee probe into the company’s dealings with specialty pharmacy Philidor RX Services uncovered accounting problems dating back to December 2014. The company said it would restate its earnings and delay filing its annual report, opening the door to a possible default on its $30 billion debt. The company has blamed some of the accounting problems on Schiller, and asked him to step down from his board seat. Schiller has refused to do so, and has denied any wrongdoing through his attorney. Valeant plans to file its annual report on April 29, two days after the hearing. Wednesday’s hearing will be primarily focused on Valeant’s drastic price increases for two of its heart drugs. The committee previously planned to vote to hold Pearson in contempt for failing to be deposed, but backed down after he agreed to be interviewed April 18.                                                                                                                                                                                                                                                                                                                                                                                                                                                      229\n",
       "Name: text, Length: 31792, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the size of every text is different\n",
    "train_x_embed.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings, test_embeddings = w2v.pool(\n",
    "    tokenized_train,\n",
    "    tokenized_test,\n",
    "    word2vec_model_google\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text_with_google_w2v(text: list[str], word2vec_model):\n",
    "    word_vectors = [word2vec_model[word] for word in text if word in word2vec_model]\n",
    "    \n",
    "    # If no words in the model, return a zero vector\n",
    "    if len(word_vectors) == 0:\n",
    "        return [np.zeros(word2vec_model.vector_size)]  \n",
    "    \n",
    "    # Return the average vector\n",
    "    return np.array(word_vectors, dtype=np.float32)\n",
    "\n",
    "\n",
    "def apply_embeddings(texts: list[str], word2vec_model):\n",
    "    embeddings = [embed_text_with_google_w2v(text, word2vec_model) for text in texts]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_embed = apply_embeddings(tokenized_train, word2vec_model_google)\n",
    "test_x_embed = apply_embeddings(tokenized_test, word2vec_model_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_embeddings(embeddings, max_length):\n",
    "    \"\"\"Pad or truncate embeddings to a fixed length\"\"\"\n",
    "    # Get embedding dimension from the first non-empty embedding\n",
    "    embedding_dim = embeddings[0].shape[1]\n",
    "    \n",
    "    # Initialize output array with zeros\n",
    "    padded = np.zeros((len(embeddings), max_length, embedding_dim), dtype=np.float32)\n",
    "    \n",
    "    # Fill the array with actual embeddings\n",
    "    for i, emb in enumerate(embeddings):\n",
    "        if len(emb) > max_length:\n",
    "            padded[i] = emb[:max_length]\n",
    "        else:\n",
    "            padded[i, :len(emb)] = emb\n",
    "            \n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31792, 15, 300), (7949, 15, 300))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_train_embeddings = pad_embeddings(train_x_embed, 15)\n",
    "padded_test_embeddings = pad_embeddings(test_x_embed, 15)\n",
    "padded_train_embeddings.shape, padded_test_embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0019272377"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_train_embeddings.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, MaxPooling1D, Concatenate, Dropout, Flatten, Embedding\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_embedding_matrix(word2vec_model, vectorizer):\n",
    "    embedding_matrix = np.zeros((len(vectorizer.vocabulary_) + 1, 300))\n",
    "    for word, idx in vectorizer.vocabulary_.items():\n",
    "        if word in word2vec_model:\n",
    "            embedding_matrix[idx] = word2vec_model[word]\n",
    "    return embedding_matrix\n",
    "\n",
    "def yoon_kim_cnn(\n",
    "    vocab_size, \n",
    "    word2vec_model,\n",
    "    vectorizer,\n",
    "    embedding_dim=300, \n",
    "    max_sequence_length=50, \n",
    "    num_classes=len(set(train_y)), \n",
    "    filter_sizes=[3, 4, 5], \n",
    "    num_filters=100\n",
    "):\n",
    "    \"\"\"\n",
    "    Implementation of Yoon Kim's CNN with three channels:\n",
    "    1. Static pre-trained word2vec\n",
    "    2. Non-static (fine-tunable) pre-trained word2vec\n",
    "    3. Random initialized embeddings\n",
    "    \"\"\"\n",
    "    # Single input for token indices\n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    \n",
    "    # Create embedding matrix from Google's word2vec\n",
    "    embedding_matrix = create_embedding_matrix(word2vec_model, vectorizer)\n",
    "    \n",
    "    # Channel 1: Static pre-trained word2vec (non-trainable)\n",
    "    embedding_static = Embedding(vocab_size, embedding_dim,\n",
    "                               weights=[embedding_matrix],\n",
    "                               input_length=max_sequence_length,\n",
    "                               trainable=False, # set this to False to keep the embeddings static\n",
    "                               name='static_channel')(sequence_input)\n",
    "    \n",
    "    # Channel 2: Non-static pre-trained word2vec (trainable)\n",
    "    embedding_non_static = Embedding(vocab_size, embedding_dim,\n",
    "                                   weights=[embedding_matrix],\n",
    "                                   input_length=max_sequence_length,\n",
    "                                   trainable=True, # set this to True to fine-tune the embeddings\n",
    "                                   name='non_static_channel')(sequence_input)\n",
    "    \n",
    "    # Channel 3: Random initialized embeddings (trainable)\n",
    "    embedding_random = Embedding(vocab_size, embedding_dim,\n",
    "                               input_length=max_sequence_length,\n",
    "                               trainable=True,\n",
    "                               name='random_channel')(sequence_input)\n",
    "    \n",
    "    # Convolutional blocks for each channel and filter size\n",
    "    conv_blocks = []\n",
    "    \n",
    "    # Apply convolutions to each channel\n",
    "    channels = [embedding_static, embedding_non_static, embedding_random]\n",
    "    channel_names = ['static', 'non_static', 'random']\n",
    "    \n",
    "    # Functional API\n",
    "    # different filters for each channel\n",
    "    for i, (channel, name) in enumerate(zip(channels, channel_names)):\n",
    "        for filter_size in filter_sizes:\n",
    "            conv = Conv1D(filters=num_filters//3,\n",
    "                         kernel_size=filter_size,\n",
    "                         padding='valid', # no padding is added to the edges of the input sequence (is used in Yoon Kim's architecture)\n",
    "                         activation='relu',\n",
    "                         name=f'conv_{name}_{filter_size}',\n",
    "                         strides=1)(channel)\n",
    "            max_pool = MaxPooling1D(pool_size=max_sequence_length - filter_size + 1,\n",
    "                                   name=f'maxpool_{name}_{filter_size}')(conv)\n",
    "            conv_blocks.append(max_pool)\n",
    "    \n",
    "    # Concatenate all pooled features\n",
    "    z = Concatenate()(conv_blocks)\n",
    "    z = Flatten()(z)\n",
    "    z = Dropout(0.5)(z)\n",
    "    \n",
    "    outputs = Dense(num_classes, activation='sigmoid')(z)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(sequence_input, outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " static_channel (Embedding)     (None, 50, 300)      480300      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " non_static_channel (Embedding)  (None, 50, 300)     480300      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " random_channel (Embedding)     (None, 50, 300)      480300      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv_static_3 (Conv1D)         (None, 48, 33)       29733       ['static_channel[0][0]']         \n",
      "                                                                                                  \n",
      " conv_static_4 (Conv1D)         (None, 47, 33)       39633       ['static_channel[0][0]']         \n",
      "                                                                                                  \n",
      " conv_static_5 (Conv1D)         (None, 46, 33)       49533       ['static_channel[0][0]']         \n",
      "                                                                                                  \n",
      " conv_non_static_3 (Conv1D)     (None, 48, 33)       29733       ['non_static_channel[0][0]']     \n",
      "                                                                                                  \n",
      " conv_non_static_4 (Conv1D)     (None, 47, 33)       39633       ['non_static_channel[0][0]']     \n",
      "                                                                                                  \n",
      " conv_non_static_5 (Conv1D)     (None, 46, 33)       49533       ['non_static_channel[0][0]']     \n",
      "                                                                                                  \n",
      " conv_random_3 (Conv1D)         (None, 48, 33)       29733       ['random_channel[0][0]']         \n",
      "                                                                                                  \n",
      " conv_random_4 (Conv1D)         (None, 47, 33)       39633       ['random_channel[0][0]']         \n",
      "                                                                                                  \n",
      " conv_random_5 (Conv1D)         (None, 46, 33)       49533       ['random_channel[0][0]']         \n",
      "                                                                                                  \n",
      " maxpool_static_3 (MaxPooling1D  (None, 1, 33)       0           ['conv_static_3[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " maxpool_static_4 (MaxPooling1D  (None, 1, 33)       0           ['conv_static_4[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " maxpool_static_5 (MaxPooling1D  (None, 1, 33)       0           ['conv_static_5[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " maxpool_non_static_3 (MaxPooli  (None, 1, 33)       0           ['conv_non_static_3[0][0]']      \n",
      " ng1D)                                                                                            \n",
      "                                                                                                  \n",
      " maxpool_non_static_4 (MaxPooli  (None, 1, 33)       0           ['conv_non_static_4[0][0]']      \n",
      " ng1D)                                                                                            \n",
      "                                                                                                  \n",
      " maxpool_non_static_5 (MaxPooli  (None, 1, 33)       0           ['conv_non_static_5[0][0]']      \n",
      " ng1D)                                                                                            \n",
      "                                                                                                  \n",
      " maxpool_random_3 (MaxPooling1D  (None, 1, 33)       0           ['conv_random_3[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " maxpool_random_4 (MaxPooling1D  (None, 1, 33)       0           ['conv_random_4[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " maxpool_random_5 (MaxPooling1D  (None, 1, 33)       0           ['conv_random_5[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 1, 297)       0           ['maxpool_static_3[0][0]',       \n",
      "                                                                  'maxpool_static_4[0][0]',       \n",
      "                                                                  'maxpool_static_5[0][0]',       \n",
      "                                                                  'maxpool_non_static_3[0][0]',   \n",
      "                                                                  'maxpool_non_static_4[0][0]',   \n",
      "                                                                  'maxpool_non_static_5[0][0]',   \n",
      "                                                                  'maxpool_random_3[0][0]',       \n",
      "                                                                  'maxpool_random_4[0][0]',       \n",
      "                                                                  'maxpool_random_5[0][0]']       \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 297)          0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 297)          0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 2)            596         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,798,193\n",
      "Trainable params: 1,317,893\n",
      "Non-trainable params: 480,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# First, convert tokenized texts to sequences of vocabulary indices\n",
    "def tokens_to_indices(tokenized_texts, vectorizer):\n",
    "    token_sequences = []\n",
    "    for text in tokenized_texts:\n",
    "        sequence = [vectorizer.vocabulary_.get(word, 0) for word in text]\n",
    "        token_sequences.append(sequence)\n",
    "    return token_sequences\n",
    "\n",
    "token_sequences = tokens_to_indices(tokenized_train, vectorizer)\n",
    "test_token_sequences = tokens_to_indices(tokenized_test, vectorizer)\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "padded_train_sequences = pad_sequences(token_sequences, maxlen=50, padding='post', truncating='post')\n",
    "padded_test_sequences = pad_sequences(test_token_sequences, maxlen=50, padding='post', truncating='post')\n",
    "\n",
    "# Create and compile the model\n",
    "model = yoon_kim_cnn(\n",
    "    vocab_size=len(vectorizer.vocabulary_) + 1, \n",
    "    word2vec_model=word2vec_model_google, \n",
    "    vectorizer=vectorizer\n",
    ")  # +1 for potential unknown tokens\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "497/497 [==============================] - 18s 17ms/step - loss: 0.0526 - accuracy: 0.9849 - val_loss: 0.0146 - val_accuracy: 0.9974\n",
      "Epoch 2/10\n",
      "497/497 [==============================] - 5s 11ms/step - loss: 0.0108 - accuracy: 0.9978 - val_loss: 0.0103 - val_accuracy: 0.9976\n",
      "Epoch 3/10\n",
      "497/497 [==============================] - 5s 11ms/step - loss: 0.0058 - accuracy: 0.9987 - val_loss: 0.0112 - val_accuracy: 0.9979\n",
      "Epoch 4/10\n",
      "497/497 [==============================] - 5s 11ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0124 - val_accuracy: 0.9977\n",
      "Epoch 5/10\n",
      "497/497 [==============================] - 5s 11ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.0142 - val_accuracy: 0.9977\n",
      "Epoch 6/10\n",
      "497/497 [==============================] - 5s 11ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.0115 - val_accuracy: 0.9979\n",
      "Epoch 7/10\n",
      "497/497 [==============================] - 5s 11ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.0192 - val_accuracy: 0.9977\n",
      "Epoch 8/10\n",
      "497/497 [==============================] - 5s 11ms/step - loss: 9.7830e-04 - accuracy: 0.9997 - val_loss: 0.0182 - val_accuracy: 0.9979\n",
      "Epoch 9/10\n",
      "497/497 [==============================] - 5s 11ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 0.0177 - val_accuracy: 0.9979\n",
      "Epoch 10/10\n",
      "497/497 [==============================] - 5s 11ms/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 0.0227 - val_accuracy: 0.9979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b630e48a90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert labels to categorical format\n",
    "\n",
    "train_y_cat = to_categorical(train_y)\n",
    "test_y_cat = to_categorical(test_y)\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_train_sequences, train_y_cat, \n",
    "          validation_data=(padded_test_sequences, test_y_cat),\n",
    "          epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249/249 [==============================] - 1s 3ms/step - loss: 0.0227 - accuracy: 0.9979\n",
      "Test Accuracy: 99.79%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(padded_test_sequences, test_y_cat)\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"NLP_yoon_kim_cnn.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tnsr-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
